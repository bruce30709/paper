<style>
.green {
  background-color: #DEFFDE;
  color: #009400;
}
.blue {
  background-color: #DEFFFF;
  color: #5959FF;
}
.red {
  color: #DC143C;
}
</style>
###### tags: `paper`
# Fault Tolerant Service Function Chaining
## Terms
<span class="green">**piggybacked**</span>
:::success
Consider a two-way transmission between host A and host B. When host A sends a data frame to B, then B does not send the acknowledgment of the frame sent immediately. The acknowledgment is delayed until the next data frame of host B is available for transmission. The delayed acknowledgment is then attached to the outgoing data frame of B. This process of delaying acknowledgment so that it can be attached to the outgoing frame is called piggybacking.
![](https://i.imgur.com/LfdEi3D.png)

:::

## 1. INTRODUCTION
In this paper, we introduce a system called fault tolerant chaining (FTC) that provides fault tolerance to an entire chain.

FTC is inspired by chain replication to efficiently provide fault tolerance. At each middlebox, FTC collects state updates due to packet processing and piggybacks them onto the packet. 

As the packet passes through the chain, FTC replicates piggybacked state updates in servers hosting middleboxes. This allows each server hosting a middlebox to act as a replica for its predecessor middleboxes. If a middlebox fails, FTC can recover the lost state from its successor servers. For middleboxes at the end of the chain, FTC transfers and replicates their state updates in servers hosting middleboxes at the beginning of the chain. FTC does not need any dedicated replica servers to tolerate f number of middlebox failures for chains with more than
f + 1 middleboxes.
## 2. BACKGROUND
Stateful middleboxes keep dynamic state for packets that they process. For instance, a stateful firewall filters packets based on statistics that it collects for network flows (Stateful firewalls can detect attempts by unauthorized individuals to access a network, as well as analyze the data within packets to see if they contain malicious code).

Middlebox state can be partitionable or shared.
* Partitionable state variables describe the state of a single traffic flow (e.g., MTU size and timeouts in stateful firewalls) and are only accessed by a single middlebox thread. 
* Shared state variables are for a collection of flows, and multiple middlebox threads query and update them (e.g., port-counts in an intrusion detection system).

A stateful middlebox is subject to both hardware and software failures that can cause the loss of its state. The root causes of these failures include 
1. bit corruptions
1. cable problems
1. software bugsserver failures due to maintenance operations 
2. power failures

We model these failures as **fail-stop** in which failures are detectable, and failed components are not restored.
### 2.1 Challenges
To recover from a middlebox failure, traffic must be rerouted to a redundant middlebox where the state of the failed middlebox is restored.

First, most middleboxes are multithreaded, and the order in which interleaving threads access shared state is nondeterministic.

One approach to accommodate non-determinism is to log any state read and write, which allows restoring any observable state from the logs.
However, this complicates the failure recovery procedure because of record/replay, and leads to high performance overheads during normal operation.

Second, to tolerate f failures, a packet is released only when at least f +1 replicas acknowledge that state updates due to processing of this packet are replicated.

In addition to increasing latency, synchronous replication reduces throughput since expensive coordinations between packet processing and state replication are required for consistency.

The overhead of this synchrony for a middlebox depends on where its replicas are located, and how state updates are transferred to these locations.
### 2.2 Limitations of Existing Approaches
Existing frameworks use one of two approaches. 
The first approach takes snapshots of middlebox state for state replication.

While taking snapshot, middlebox operations are stalled for consistency. These frameworks take snapshots at different rates.

They take snapshots per packet or packet-batch introducing 400 µs to 8–9 ms of per packet latency overhead. Periodic snapshots (e.g., at every 20–200 ms intervals) can cause periodic latency spikes up to 6 ms. We measure that per middlebox snapshots cause 40% throughput drop going from a single middlebox to a chain of five middleboxes.

The second approach redesigns middleboxes to separate and push state into a fault tolerant backend data store. This separation incurs high performance penalties. Accessing state takes at least a round trip delay. Moreover, a middlebox can release a packet only when it receives an acknowledgement from the data store that relevant state updates are replicated. Due to such overheads, the middlebox throughput can drop by ∼60% and reduce to 0.5 Gbps (for packets with 1434 B median size).
## 3. SYSTEM DESIGN OVERVIEW

### 3.1 Requirements
FTC adheres to four requirements:
1. Correct recovery: FTC ensures that the middlebox behavior after a failure recovery is consistent with the behavior prior to the failure. To tolerate f failures, a packet can only be released outside of a chain once all necessary information needed to reconstruct the internal state of all middleboxes is replicated to f + 1 servers.
1. Low overhead and fast failure recovery: Fault tolerance for a chain must come with low overhead. A chain processes a high traffic volume and middlebox state can be modified very frequently. At each middlebox of a chain, latency should be within 10 to 100 µs, and the fault tolerance mechanism must support accessing variables 100 k to 1 M times per second. Recovery time must be short enough to prevent application outages. For instance, highly available services timeout in just a few seconds.
1. Resource efficiency: Finally, the fault tolerance solution should be resource efficient. To isolate the effect of possible failures, replicas of a middlebox must be deployed on separate physical servers. We are interested in a system that dedicates the fewest servers to achieve a fixed replication factor.

### 3.2 Design Choices
We model packet processing as a transaction. FTC carefully collects updated values of state variables modified during a packet transaction and appends them to the packet. As the packet passes through the chain, FTC replicates piggybacked state updates in servers hosting the middleboxes.
* Transactional packet processing:
    To accommodate non-determinism due to concurrency, we model the processing of a packet as a transaction, where concurrent accesses to shared state are serialized to ensure that consistent state is captured and replicated.

    This model is easily adaptable to hybrid transactional memory, where we can take advantage of the hardware support for transactions. This allows FTC to use modern hardware transactional memory for better performance, when the hardware is present.
* In-chain replication:
A high-availability cluster approach requires f + 1 replicas as it relies on a fault tolerant coordinator for failure detection.  
* State piggybacking:
To replicate state modified by a packet, existing schemes send separate messages to replicas. 
* No checkpointing and no replay:
FTC replicates state values at the granularity of packet transactions, rather than taking snapshots of state or replaying packet processing operations. 
* Centralized orchestration:
In our system, a central orchestrator manages the network and chains. The orchestrator deploys fault tolerant chains, reliably monitors them, detects their failures, and initiates failure recovery. 
## 4. FTC FOR A SINGLE MIDDLEBOX
### 4.1 Middlebox State Replication
FTC uses sequence numbers, similar to TCP, to handle out-of-order deliveries and packet drops within the network.
![](https://i.imgur.com/tVUETIV.png)
Figure 2 shows our protocol for providing fault tolerance for a middlebox. FTC replicates the middlebox state in f + 1 replicas during normal middlebox operations. Replicas r1, . . . ,rf +1
form the replication group for middlebox m where r1 and rf +1 are called the head and tail replicas.
Each replica is placed on a separate server whose failure is isolated. With state replicated in f + 1 replicas, the state remains available even if f replicas fail.

**Normal operation of protocol:** As shown in Figure 2, the middlebox processes a packet, and the head constructs and appends a piggyback log to the packet. The piggyback log contains a sequence number and a list of state updates during packet processing. As the packet traverses the chain, each subsequent replica replicates the piggyback log and applies the state updates to its state store. After replication, the tail strips the piggyback log and releases the packet.

**Correctness:** Each replica replicates the per-packet state updates in order. As a result, when a replica forwards a packet, it has replicated all preceding piggyback logs.Packets also pass through the replication group in order. When a packet reaches a replica, prior replicas have replicated the state updates carried by this packet.
Thus, when the tail releases a packet, the packet has already traversed the entire replication group. The replication group has f + 1 replicas allowing FTC to tolerate f failures.

**Failure recovery:** FTC relies on a fault tolerant orchestrator to reliably detect failures. Upon failure detection, the replication group is repaired in three steps: adding a new replica, recovering the lost state from an alive replica, and steering traffic through the new replica. 

If the head fails, the new replica retrieves the state store, piggyback logs, and sequence number from the immediate successor to the head. If other replicas fail, the new replica fetches the state
from the immediate predecessor.

**log propagation invariant:** for each replica except the tail, its successor replica has the same or prior state, since piggyback logs propagate in order through the chain.

To ensure that the log propagation invariant holds during recovery, the replica that is the source for state recovery discards any out-of-order packets that have not been applied to its state store and will no longer admit packets in flight. If the contacted replica fails during recovery, the orchestrator detects this failure and re-initializes the new replica with the new set of alive replicas.

Finally, the orchestrator updates routing rules in the network to steer traffic through the new replica. If multiple replicas have failed, the orchestrator waits until all new replicas acknowledge that they have successfully recovered the state. Then, the orchestrator updates the necessary routing rules from the tail to the head.

### 4.2 Concurrent Packet Processing
To achieve higher performance, we augment our protocol to support multithreaded packet processing and state replication in the middlebox and the head. Other replicas are still single threaded.

**Transactional Packet Processing:** In concurrent packet processing, the effects on state variables must be serializable. Further, state updates must be applied to replicas in the same order so that the system can be restored to a consistent state during failover. To support this requirement, replay based replication systems, such as FTMB, track all state accesses, including state reads, which can be challenging to perform efficiently.

In transactional packet processing, state reads and writes by a packet transaction have no impact on another concurrently processed packet. This isolation allows us to only keep track of the relative order between transactions, without needing to track all state variable dependencies.

We realize this model by implementing a software transactional memory (STM) API for middleboxes. When a packet arrives, the runtime starts a new packet transaction in which multiple reads and writes can be performed. Our STM API uses fine grained strict two phase locking to provide serializability. Our API uses a wound-wait scheme that aborts transaction to prevent possible deadlocks if a lock ordering is not known in advance. An aborted transaction is immediately re-executed. The transaction completes when the middlebox releases the packet.

Using two phase locking, the head runtime acquires necessary locks during a packet transaction. We simplify lock management using **state space partitioning**, by using the hash of state variable keys to map keys to partitions, each with its own lock. The state partitioning is consistent across all replicas, and to reduce contention, the number of partitions is selected to exceed the maximum number of CPU cores.

At the end of a transaction, the head atomically increments its sequence number only if state was updated during this packet transaction. Then, the head constructs a piggyback log containing the state updates and the sequence number. After the transaction completes, the head appends the piggyback log to the packet and forwards the packet to the next replica.

### 4.3 Concurrent State Replication
Up to now FTC provides concurrent packet processing but does not support concurrent replication. The head uses a single sequence number to determine a total order of transactions that modify state partitions. This total ordering eliminates multithreaded replication at successor replicas.

Data dependency vectors: We use data dependency vectors to determine a partial order of transactions in the head. Each element of this vector is a sequence number associated to a state partition. A packet piggybacks this partial order to replicas enabling them to replicate transactions with more concurrency; a replica can apply and replicate a transaction in a different serial order that is still equivalent to the head.

Each successor replica keeps a dependency vector MAX that tracks the latest piggyback log that it has replicated in order, i.e., it has already received all piggyback logs prior to MAX. In case a packet is lost, a replica requests its predecessor to retransmit missing piggyback logs.

![](https://i.imgur.com/RxstEDr.png)

1. The head performs a packet transaction that writes to state partition 1 and increments the associated sequence number. The piggyback log belonging to this transaction contains “don’t care” value for state partitions 2 and 3 (denoted by x), since the transaction did not read or write these partitions.
1. The head performs another transaction and forwards the packet with a piggyback log. 
1. As shown the second packet arrives to the replica before the first packet. Since the piggybacked dependency vector is out of order, the replica holds the packet. 
1. The first packet arrives. Since the piggybacked vector is in order, the replica applies the piggyback log and updates its local dependency vector accordingly.
1. By applying the piggyback log of the first packet, the replica now can apply the piggyback log of the held packet.

## 5. FTC FOR A CHAIN
![](https://i.imgur.com/TSbx5eS.png)

1. The forwarder appends a state message containing the state updates of the last f middleboxes in the chain from the buffer to an incoming packet, and forwards this packet to the first replica. 
1. Each replica applies the piggybacked state updates, allows the middlebox to process the packet, and appends new state updates to the packet.
1. Replicas at the beginning of the chain replicate for middleboxes at the end of the chain. The buffer holds packets and releases them once state updates from the end of the chain are replicated.
1. The buffer transfers the piggyback message to the forwarder that adds it to incoming packets for state replication.

Our protocol for a chain enables each middlebox to replicate the chain’s state while processing packets. To accomplish this, we extend the original chain replication protocol during both normal operation and failure recovery. FTC supports middleboxes with different functionalities to run across the chain, while the same process must be running across the nodes in the original chain replication protocol. FTC’s failure recovery instantiates a new middlebox at the failure position to maintain the chain order, while the traditional protocol appends a new node at the end of a chain.

Our protocol can be thought of as running n instances (per middlebox)
of the protocol developed earlier in § 4. FTC places a replica per each middlebox. Replicas form n replication groups, each of which provides fault tolerance for a single middlebox.

Viewing a chain as a logical ring, the replication group of a middlebox consists of a replica and its f succeeding replicas. Instead of being dedicated to a single middlebox, a replica is shared among f + 1 middleboxes and maintains a state store for each of them. Among these middleboxes, a replica is the head of one replication group and the tail of another replication group. A middlebox and its head are co-located in the same server. For instance in Figure 4, if f = 1 then the replica r1 is in the replication groups of middleboxes m1 and mn, and r2 is in the replication groups of m1 and m2. Subsequently, the replicas rn and r1 are the head and the tail of middlebox mn.

FTC adds two additional elements, the forwarder and buffer at the ingress and egress of a chain. The forwarder and buffer are also multithreaded, and are collocated with the first and last middleboxes. The buffer holds a packet until the state updates associated with all middleboxes of the chain have been replicated. The buffer also forwards state updates to the forwarder for middleboxes with replicas at the beginning of the chain. The forwarder adds state updates from the buffer to incoming packets before forwarding the packets to the first middlebox. 
### 5.1 Normal Operation of Protocol
The forwarder receives incoming packets from the outside world and piggyback messages from the buffer. A piggyback message contains middlebox state updates. As the packet passes through the chain, a replica detaches and replicates the relevant parts of the piggyback message and applies associated state updates to its state stores. A replica ri tracks the state updates of a middlebox mi and updates the piggyback message to include these state updates. Replicas at the beginning of the chain replicate for middleboxes at the end of the chain. The buffer withholds the packet from release until the state updates of middleboxes at the end of the chain are replicated. The buffer transfers the piggyback message to the forwarder that adds it to incoming packets for state replication.

**Other considerations:** There may be time periods that a chain receives no incoming packets. In such cases, the state is not propagated through the chain, and the buffer does not release packets. To resolve this problem, the forwarder keeps a timer to receive incoming packets. Upon the timeout, the forwarder sends a propagating packet carrying a piggyback message it has received from the buffer. Replicas do not forward a propagating packet to middleboxes. They process and update the piggyback message as described before and forward the packet along the chain. The buffer processes the piggyback message to release held packets.
Some middlebox in a chain can filter packets (e.g., a firewall may block certain traffic), and consequently the piggybacked state is not passed on. For such a middlebox, its head generates a propagating packet to carry the piggyback message of a filtered packet. Finally, if the chain length is less than f +1, we extend the chain by adding more replicas prior to the buffer. These replicas only process and update piggyback messages.
### 5.2 Failure Recovery
Once the state is recovered, the new replica notifies the orchestrator to update routing rules to steer traffic through the new replica. For simultaneous failures, the orchestrator waits until all new replicas confirm that they have finished their state recovery procedures before updating routing rules.

## 6. EVALUATION
![](https://i.imgur.com/YaSsrUH.png)


## 7. EVALUATION
![](https://i.imgur.com/nB8Dg0I.png)
![](https://i.imgur.com/jHzDkjA.png)
![](https://i.imgur.com/ZketKF0.png)

