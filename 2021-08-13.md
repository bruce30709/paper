<style>
.green {
  background-color: #DEFFDE;
  color: #009400;
}
.blue {
  background-color: #DEFFFF;
  color: #5959FF;
}
.red {
  color: #DC143C;
}
</style>
###### tags: `paper`
# NFP: Enabling Network Function Parallelism in NFV
## Term
<span class="green">**Horizontally VS Vertically Accelerate**</span>
:::success
![](https://i.imgur.com/OIdn0Vn.png)
:::

## 1. INTRODUCTION 
**1. Individual NF acceleration:** Offload software logic onto programmable hardware to accelerate individual NFs.

**2. Packet delivery acceleration:** Optimize packet delivery from the network interface cards (NICs) to VMs, and between VMs.

**3. NF modularization:** Modularizes NFs and improves overall performance by sharing common building blocks between NFs and chaining the remaining blocks together.

• We present the motivation and design challenges of introducing NF parallelism into NFV, and propose the NFP framework that exploits NF parallelism to improve NFV performance. 

• NFP provides a policy specification scheme for intuitively representing equential or parallel chaining intents of network operators to improve the parallelism optimization effect of NFP. 

• We design NFP orchestrator that can identify NF dependency and automatically compile policies into high performance service graphs with parallel NFs. 

• We design NFP infrastructure that efficiently supports NF parallelism based on light-weight packet copying, distributed parallel packet delivery, and load-balanced merging of packet copies. 

• We implement NFP based on DPDK in Linux containers. Evaluations show that NFP could achieve up to 35.9% latency reduction for real world service chains.
## 2. MOTIVATION AND CHALLENGES
### 2.1 Background and Motivation
**pipelining model:** uses multiple cores to carry a chain.

**run-to-completion (RTC) model:** consolidates an entire service chain as a native process on a CPU core.

**NF Parallelism brings significant latency benefit:** 
Based on the statistics, we find that 53.8% of NF pairs can be parallelized.

**NF Parallelism can work with and benefit other optimization techniques:** 
1. For individual NF acceleration techniques, NF parallelism can parallelize independent accelerated NFs to achieve higher performance.
2. We can use fast packet delivery technologies to accelerate packet delivery in NF parallelism.
3. NF parallelism could benefit both monolithic and modularized NFs. After decomposing NFs into building blocks, common modules can be shared, and NF parallelism can be implemented in the granularity of building blocks.

### 2.2  Design Challenges
**Policy design to describe service graphs** 
When we intend to support NF parallelism in NFV, traditional approach for specifying NF positions cannot be used to describe NF parallelism intents. 
Therefore, supporting NF parallelism requires a new, intuitive way to describe both sequential and parallel NF composition intents to construct optimized service graphs.

**Orchestrator design to construct service graphs:** 
Supporting NF parallelism challenges the orchestrator to identify NF dependencies and automatically compile NFP policies into high performance service graphs.
We propose NF dependency principles along with an automatic dependency identification algorithm running in the NFP orchestrator.

**Orchestrator design to optimize resource overhead:** 
NF parallelism may introduce two or more copies of every packet that could occupy extra network bandwidth resource and largely deteriorate throughput. Thus, the orchestrator is challenged to construct high performance service graphs with marginal resource overhead.
We carefully design the NFP orchestrator that intelligently identifies opportunities to realize NF parallelism without packet copying.

**Infrastructure design to support NF parallelism:** 
First, the infrastructure should support light-weight packet copying to minimize the copy overhead. Second, the infrastructure requires a merging module to merge processed packets from parallelized NFs into the final output. 
Finally, current solutions on packet delivery between NFs  depend on a centralized virtual switch. However, packet queuing in this centralized switch would compromise the performance. 
We introduce the NFP infrastructure design to address above challenges.
## 3. POLICY DEFINITION
![](https://i.imgur.com/TVFNuys.png)
We define a policy specification scheme, which includes three types of rules, in NFP.
**Order (NF1, before, NF2):**  
This rule expresses the desired execution order of two NFs. For example, in the service chain shown in Figure 1(a), the network operator can first send the traffic to the VPN and then the Monitor by specifying Order(VPN, before, Monitor). 
This ensures the compatibility of NFP to support sequential service chains. 
**Priority (NF1 > NF2):**  
The actions of the two NFs may conflict.
Therefore, network operators can specify the Priority(IPS > Firewall) rule to parallelize the two NFs while indicating the system should adopt the processing result of IPS during conflicts.
An Order rule is used to intuitively describe sequential NF composition intents, while two NFs in a Priority rule are intended to be executed in parallel.
NFP orchestrator further inspects the dependency of NFs in an Order rule to see whether they are parallelizable. If they are, an Order rule is converted into a Priority rule, and the NF with the back order is assigned a higher priority. 
**Position (NF, first/last):**  
We cannot preacknowledge the final optimized graph structure. Thus, we can only assign an NF as the first or last one in the service graph. We design the Position(NF, first/last) rule to describe such type of intents.
With above rules, network operators can define chaining intents by composing multiple rules into a policy (the third row in Table 1) to describe a service graph (Figure 1(b)).
## 4. ORCHESTRATOR DESIGN
### 4.1 NF Parallelism Analysis
We observe that the actions of different NFs may conflict with each
other. 
we propose a result correctness principle: *Two NFs can work in parallel, if parallel execution of the two NFs results in the same processed packet and NF internal states as the sequential service composition*.
![](https://i.imgur.com/SfHOSam.png)
![](https://i.imgur.com/t4S95ZG.png)

### 4.2 Resource Overhead Optimization
In a Priority rule or a parallelizable Order rule, the two NFs can work in parallel with packet copying, which could incur resource overhead. Furthermore, large memory block copying could degrade latency and throughput performance.
**OP#1: Dirty Memory Reusing:**  
If two NFs read or write different fields of a packet, they can operate on the same packet copy. We name this optimization technique as Dirty Memory Reusing, which could reduce packet copying necessities.
According to our evaluation in § 6, despite the possible existence of cache contention, by adopting NF parallelism, NFP could still significantly outperform sequential service chaining in NFV.
**OP#2: Header-Only Copying:**  
We observe from Table 2 that only few NFs (7%) modify packet payloads.
Therefore, we propose Header-Only Copying that only copies packet headers for some cases of NF parallelism. 
Header-Only Copying could improve performance and save memory by shortening the length of memory to be copied.
### 4.3 NF Parallelism Identification Algorithm
![](https://i.imgur.com/WQkU1Pj.png)
The algorithm can determine that the two NFs can be parallelized without packet copying or with packet copying, or cannot be parallelized. First, the algorithm fetches all the actions of the two NFs from AT (lines 1-2). Then it exhaustively goes over all action pairs from the two NFs (lines 5-17) to figure out the parallelism possibility for the two NFs based on the DT.
For the read-write or write-write case, we need to further decide if the two actions operate on the same field (lines 6-9). If the two NFs can be parallelized with packet copying, we need to record the conflicting actions (lines 16-17). Finally, the algorithm generates the output of whether the two NFs are parallelizable (p) and possible conflicting actions (ca), whose existence indicates the necessity of packet copying.
### 4.4 Service Graph Construction
It first transforms policies into pre-defined intermediate representations, then compiles the intermediate representations into independent micrographs, and finally merges the micrographs to generate the final service graph.
![](https://i.imgur.com/IWEB2fP.png)
#### 4.4.1 Transforming Policies into Intermediate Representations.
We design two types of intermediate representations to store NFP policies, as shown in Figure 2. For Position rules, we maintain the NF type and its position in the left representation block, which records the placement of a single NF. For Order rules, we implement the Algorithm 1 to check whether they can be parallelized and identify conflicting actions. For Priority rules, we still need Algorithm 1 to find out conflicting actions. 
#### 4.4.2 Compiling Intermediate Representations into Micrographs.
we first sequentially chain NFs that are not parallelizable (e.g. NF2 and NF3 in Figure 2). Then we concatenate intermediate representations with overlapping NFs into a micrograph by using overlapping NFs as junction points. There are three types of micrograph structures including Single NF (e.g. NF1, NF8), Tree (e.g. NF2, NF3 and NF4), and Plain Parallelism (e.g. NF5, NF6 and NF7). Single NF micrographs come from NFs assigned in Position rules (e.g. NF1), or free NFs with no rule restrictions (e.g. NF8). Tree micrographs come from unparallelizable NFs.
#### 4.4.3 Merging Micrographs into the Final Graph.
Finally, we merge micrographs to generate the service graph. NFs assigned by Position rules are first placed in the head/tail of the chain (NF1).
Then we wrap up each remaining micrograph (including free NFs) as one NF, and exhaustively check the dependency of each micrograph pair to decide their parallelism.
Based on the final graph structure, NF dependencies, and NF priorities, we create a classification table that records how to direct a packet to its corresponding service chain, a forwarding table that records how to steer different packet copies (version1 and version2 in Figure 2), and a merging table that stores how to merge packet copies.
## 5. INFRASTRUCTURE DESIGN
NFP adopts consolidation to avoid occupying extra network bandwidth resource.
We use the zero-copy packet delivery proposed in NetVM.
As shown in Figure 3, each NF owns a receive ring buffer and a transmit ring buffer, which are stored in a shared memory region allocated in huge pages accessible to all NFs.

However, the infrastructure design for supporting NF parallelism
is challenged in several aspects.
• NFs may drop packets. The infrastructure is challenged to deal with the situation where one of the two parallel NFs drops the packet, and the other one reads or modifies it.
• The infrastructure needs to merge multiple versions of a packet to create the final output, which incurs the challenge for the merging module to handle heavy load without becoming a performance bottleneck.
• In previous work, packet steering among NFs relies on a centralized virtual switch, which according to our evaluation incurs a performance overhead due to packet queuing. The infrastructure requires a more efficient approach in delivering packets among NFs.

When a packet enters an NFP server, we introduce a classifier that sends the packet reference into the proper service graph. For the packet delivery, we design a distributed NF runtime to efficiently deliver packets among NFs in parallel. Finally, multiple copies of a packet are sent into the merger module to generate the final output.
![](https://i.imgur.com/znDykeM.png)
### 5.1 Packet Classification
The classifier module takes an incoming packet from the NIC and finds out the corresponding service graph information for the packet, including how many packet copies are expected in the merger, how to merge different copies of a packet, and the first hop(s) of the service graph.

Therefore, the classifier maintains a Classification Table (CT) shown in Figure 4 to store the match fields (e.g. five tuple), the total packet copy count to be received in the merger, the merging operations (MOs) to merge packet copies (details are presented in §5.3), and the actions indicating the first NF(s) of the service graph, based on which the classifier sends the packet into the entrance of the graph.

Note that different packets in a flow, or different flows that follow the same service graph are forwarded and merged in the same pattern according to its service graph structure. Therefore, we tag those packets that follow the same service graph with the same Match ID (MID) to avoid repeated storage of the service graph information. Latter modules could identify the service graph to which the packet belongs based on MID to forward or merge packets. To transmit the MID to latter modules, we tag it into packet metadata shown in Figure 5. Twenty bits of MID could express 1M service graphs.

However, despite each packet in a flow should be merged in the same way, the merger needs to collect all versions of each packet to generate the output for this packet.
Therefore, the NFP classifier attaches a 64 bits metadata to a packet, recording the MID, PID and version of a specific packet copy.

Each Classification Table entry is generated by the orchestrator to direct a flow into a specific service graph.
![](https://i.imgur.com/WYfVUzF.png)
![](https://i.imgur.com/8WiTd4c.png)

### 5.2 Packet Delivery Among NFs
After an NF processes a packet, NFP should steer the packet to the subsequent NFs in the service graph without copying, or copy the packet and send the copies into parallel NFs. As mentioned above, using a centralized virtual switch as the forwarder might incur performance overhead. 

To address this challenge, NFP distributes the packet forwarding task and enables each NF to independently forward packets to subsequent NFs in parallel.

To make this process transparent to NF developers and incur no NF modifications, we design an NF runtime for each NF to perform traffic steering, as shown in Figure 3. After packet processing, the NF could delegate the packet to the NF runtime, which copies the packet reference
to the next NFs’ ring buffer to realize packet forwarding. Through the distributed NF runtime, we could parallelize the packet delivery process and alleviate the forwarding hot spot.

Each NF runtime maintains a forwarding table (FT) (see Figure 4), which stores a local view of the entire service graph. The global forwarding table is generated at the end of the service graph construction process (§4.4.3), and then statically installed to the Chaining Manager (as shown in dashed blue line in Figure 3). The chaining Manager splits the global table and installs the forwarding rules to each NF runtime. When an NF delegates a packet to the NF runtime, the MID in the packet metadata is used to look up the actions in FT. We design four types of actions.

**ignore:** 
When an NF intends to drop the packet, it conveys the dropping intention to the NF runtime. The NF runtime then implements the ignore action to ignore original actions in the FT entry for this packet. Furthermore, the NF runtime sends a nil packet to deliver the dropping intention to the merger.
**distribute(version, targets):**
This action sends the reference of a specific version of a packet to one or multiple target parallel NFs without packet copying.
**copy(version1, version2):**
This action copies packet version1 and tags the new copy as version2. We only copy packet headers and set the “packet length” field as the length of the header itself. Besides, we prepare memory blocks to store input or copied packets during the system initialization. Therefore, the header copying does not require dynamic allocation of the memory and could avoid performance degradation.
**output(version):**
This action is used to output the packet after it has traversed the entire service graph. The output action is performed by the last NF in the service graph possibly assigned by a Position rule.
Note that the actions field of the Classification Table also includes above actions. The classifier may copy and send a packet into one or more NFs as a start, according to the service graph structure.
### 5.3 Load Balanced Packet Merging
After all NFs have processed a packet, multiple copies of this packet is sent into a merger module to generate the final output.
The merger maintains a dynamic Accumulating Table (AT) as shown in Figure 4. Each entry records the received packet copy count and the received packet copy versions of a packet. 
![](https://i.imgur.com/4N7YPkE.png)
**Packet Merging:** 
When the current count field in AT reaches the total count recorded in CT referenced by the key of MID, the merger will merge the packet copies according to the merging operations (MOs) in the CT.
In other words, MOs indicate which bits of different packet versions should be included in the final processed packet. MOs include three types of operations.
• modify(v1.A, v2.A)
• add(v2.B, before/after, v1.A)
• remove(v1.C)
**Merger Load Balancing:** 
The merger is heavily burdened to process all copies of every packet and could introduce performance bottleneck.
we propose to deploy multiple mergers in one NFP server and design a merger agent to balance the load among the instances. A merger instance maintains a local AT, and could merge any packet from any service graph.
### 5.4 Integrating Network Functions into NFP
To integrate a new NF into the system, NFP needs the actions of the NF for parallelism identification and service graph construction. To this end, NFP provides an inspection tool for operators that can inspect NF codes to find the usage of interfaces that operate on packets, including reading, writing, dropping and adding/removing bits. Operators can run the inspector against their NF code to automatically generate an action profile, which can be registered into NFP to integrate the new NF into NFP.
## 6. IMPLEMENTATION AND EVALUATION
We implement the NFP framework and the NF action inspector (14,000 LoC in total) based on DPDK version 16.11.
We run NFs using Docker. Each NF runs inside a container on a physical CPU core.
We isolate and dedicate each core to a container alone, which could ensure that the core will not be scheduled by the operating system.
The classifier also consumes a separate CPU core.
For test traffic, we use a DPDK based packet generator that runs on a separate server and is directly connected to the test server.
The generator sends and receives traffic to measure the latency and the maximum throughput without packet loss.
### 6.1 Network Functions
**L3 Forwarder**
**Load Balancer**
**Firewall**
**IDS**
**VPN**
**Monitor**
### 6.2 Performance Improvement
![](https://i.imgur.com/TC10gjY.png)
![](https://i.imgur.com/Rc1yPGQ.png)
![](https://i.imgur.com/YE0LMnF.png)
![](https://i.imgur.com/qdNUVdp.png)
![](https://i.imgur.com/9VTLIyV.png)

### 6.3 Overhead
![](https://i.imgur.com/ttvrxU3.png)
![](https://i.imgur.com/E4wUYpZ.png)
![](https://i.imgur.com/ZWXAZrZ.png)

### 6.4 Real World Service Chains

## 7 DISCUSSION
![](https://i.imgur.com/uYrmsVB.png)
![](https://i.imgur.com/9AtRsPM.png)


