<style>
.green {
  background-color: #DEFFDE;
  color: #009400;
}
.blue {
  background-color: #DEFFFF;
  color: #5959FF;
}
.red {
  color: #DC143C;
}
</style>
###### tags: `paper`
# NFVnice: Dynamic Backpressure and Scheduling for NFV Service Chains
## Terms
<span class="green">**backpressure**</span>
:::success
當publisher/producer發出的資料量超過subscriber/consumer時的情況，換句話說，當subscriber/consumer無法負荷來自publisher/producer發出的資料量時就會形成backpressure。
![](https://i.imgur.com/FCV1Vnt.png)
:::

## I. INTRODUCTION
Just as hardware switches and routers provide rate-proportional scheduling for packet flows, an NFV platform has to provide a fair processing of packet flows.

Secondly, the tasks running on the NFV platform may have heterogeneous processing requirements that OS schedulers (unlike hardware switches) address using their typical fair scheduling mechanisms. OS schedulers, however, do not treat packet flows fairly in proportion to their arrival rate. Thus,
NF processing requires a re-thinking of the system resource management framework to address both these requirements.

Moreover, standard OS schedulers: a) do not have the right metrics and primitives to ensure fairness between NFs that deal with the same or different packet flows; b) do not make scheduling decisions that account for chain level information; and c) cannot guarantee predictable per-flow latency
requirements. 

Additionally, processor performance is critically dependent on cache performance, which in turn depends on locality of reference.
When the OS switches contexts, locality of access may not occur because the instructions and data of the newly-scheduled NF may no longer be in the cache(s). Context switching results in additional NF processing costs, beyond the typical cost associated with the operations performed by the kernel and have to be accounted for.

NFV platforms are necessarily different because: a) the OS scheduler does not know a priori, the capacity or processing requirements for each NF; b) an NF may have variable per-packet costs (e.g., some packets may trigger DNS lookup, which are expensive to process, and others may just be an inexpensive
header match).

• Automatically tuning CPU scheduling parameters to provide a fair allocation that weighs NFs based on both their packet arrival rate and the required computation cost.
• Determining when NFs are eligible to get a CPU share and when they need to yield the CPU, entirely from user space, improving throughput and fairness regardless of the kernel scheduler being used.
• Leveraging the scheduling flexibility to achieve backpressure for service chain-level congestion control, that avoids unnecessary packet processing early in a chain if the packet might be dropped later on.
• Extending backpressure to apply not only to adjacent NFs in a service chain, but for full service chains and managing congestion across hosts using ECN.
• Presenting a scheduler-agnostic framework that does not require any operating system or kernel modifications.
## II. BACKGROUND AND MOTIVATION
### A. Diversity, Fairness, and Chain Efficiency

Fair Scheduling: Determining how to allocate CPU time to
network functions in order to provide fair and efficient chain
performance despite NF diversity is the focus of our work.

we apportion the resources (CPU cycles) to NFs based on the combination of each NF’s arrival rate and processing cost.
Intuitively, if either one of these factors is fixed, then we expect its CPU allocation to be proportional to the other metric. For example, if two NFs have the same computation cost but one has twice the arrival rate of the other, then it must have twice the output rate relative to the second NF. Alternatively, if the NFs have the same arrival rate, but one requires twice the processing cost, then we expect the heavy NF to get twice as much CPU time, resulting in both NFs having the same output rate.

Unfortunately, standard CPU schedulers do not have sufficient information to allocate resources in a way that provides rate-cost proportional fairness. CPU schedulers typically try to provide fair allocation of processing time, but if computation costs vary between NFs this cannot provide rate-cost fairness.

i) it ensures that all competing NFs get a minimal CPU share necessary to make progress even in the worst case scenario (highly uneven and overloaded across competing NFs),while seeking to maximize the throughput for a given load across allthe NFs.
ii) the rate-cost proportional fairness is general and flexible, so that it can be tuned to meet the QoS policies desired by the operator.

Efficient Chaining: Beyond simply allocating CPU time fairly to NFs on a single core, the combination of NFs into service chains demands careful resource management across the chain to minimize the impact of bottlenecks. 

When an NF (whether a single NF or one in a service chain) is overloaded, packet drops become inevitable, and processing resources already consumed by those packets are wasted. 

For responsive flows, such as TCP, congestion control and avoidance using packet drop methods such as RED, REM, SFQ, CSFQ and feedback with Explicit Congestion Notification (ECN) can cause the flows to adapt their rates to the available capacity on an end-to-end basis. However, for non-responsive flows (e.g., UDP), a local, rapidly adapting method is backpressure, which can propagate information regarding a congested resource upstream (to previous NFs in the chain). It is important however to ensure that effects such as head-of-the-line blocking or unfairness do not creep in as a result.

### B. Existing OS Schedulers Are Ill-Suited for NFV Deployment
Linux provides several different process schedulers, with the Completely Fair Scheduler (CFS) being the default since kernel 2.6.23.

Thus, CFS ensures a fair proportion of CPU allocation to all the tasks. The CFS Batch variant has fewer timer interrupts than normal CFS, leading to a longer time quantum and fewer context switches, while still offering fairness.The Round Robin (RR) scheduler (part of the linux real-time (RT) scheduling class), simply cycles through processes with a specified time quantum (1-100ms), but does not focus on a particular measure of fairness other than equal allocation of cycles.

We consider three heterogeneous NFs (computation costs: NF1 = 500, NF2 = 250 and NF3 = 50 CPU cycles) subject to equal and unequal loads. Figure 1 shows that when arrival rates are the same, none of the schedulers are able to provide our fairness goal—an equal output rate for all three NFs.

![](https://i.imgur.com/zZYQp5B.png)
CFS Normal always apportions CPU equally, regardless of offered load and NF processing cost, so the lighter weight NF3 gets the highest throughput. The RR scheduler gives each NF an equal chance to run, but does not limit the time the NF runs for. The CFS Batch scheduler is in between these extrees since it seeks to provide fairness, but over longer time periods.

![](https://i.imgur.com/nShV8Pf.png)
Figure 2 shows the box plot of the latency seen with different schedulers. The choice of scheduler has significant impact on the latency. Moreover, the variance (min, max, and the three quartiles) in latency is much higher with the CFS (Normal and Batch) schedulers that perform more frequent context switches compared to the RR schedulers (1ms or 100ms).


## III. DESIGN AND IMPLEMENTATION
In an NFV platform, at the top of the stack are one or more network functions that must be scheduled in such a way that idle work (i.e., while waiting for packets) is minimized and load on the service chain is shed as early as possible so as to avoid wasted work. However, the operating system’s process scheduler that lies at the bottom of the software stack remains completely application agnostic, with its goal of providing a fair share of system resources to all processes.

The design of NFVnice centers around the concept of assisted preemptive scheduling, where network functions provide hints to the underlying OS with regard to their utilization. In addition to monitoring the average computation time of a network function per packet, NFVnice needs to know when NFs in a chain are overloaded, or blocked on packet/disk I/O.

NFVnice leverages cgroups, a standard userspace primitive provided in linux to manipulate process scheduling. NFVnice monitors queue sizes, computation times and I/O activities in user space with the help of libnf and manipulates scheduling weights accordingly.

### A. System Components
![](https://i.imgur.com/XN99Zrz.png)


Figure 3 illustrates the key components of the NFVnice platform. We leverage DPDK for fast userspace networking.

Our NFV platform is implemented as a system of queues that hold packet descriptors pointing to shared memory regions.

The NF Manager runs on a dedicated set of cores and is responsible for ferrying packet references between the network interface card (NIC) queues and NF queues in an efficient manner. When packets arrive to the NIC, Rx threads in the NF Manager take advantage of DPDK’s poll mode driver to deliver the packets into a shared memory region accessible to all the NFs.
The Rx thread does a lookup in the Flow Table to direct the packet to the appropriate NF. Once a flow is matched to an NF, packet descriptors are copied into the NF’s receive ring buffer and the Wakeup subsystem brings the NF process into the runnable state. After being processed by an NF, the NF Manager’s Tx threads move packets through the remainder of the chain. This provides zero-copy packet movement.

When an NF finishes with a packet, it enqueues it in its Tx queue, where it is read by the manager and redirected to the Rx queue of the next NF in the chain. The NF Manager also picks up packets from the Tx queue of the last NF in the chain, and sends it out over the network.

The NF Manager’s scheduling subsystem determines when an NF should be active
and how much CPU time it should be allocated relative to other NFs. The backpressure subsystem provides chain-aware management, preventing NFs from spending time processing packets that are likely to be dropped downstream.
System Management and NF Deployment: The NF Manager ’s (Rx, Tx and Monitor) threads are pinned to separate dedicated cores. The number of Rx, Tx and monitor threads are configurable (C-Macros), to meet system needs, and available CPU resources.

### B. Scheduling NFs
Each network function in NFVnice is implemented inside its own process (potentially running in a container). Thus the OS scheduler is responsible for picking which NF to run at any point in time. We believe that rather than design an entirely new scheduler for NFV, it is important to leverage Linux’s existing scheduling framework, and use our management framework in user space to tune any of the stock OS schedulers to provide the properties desired for NFV support.
![](https://i.imgur.com/tHgdaDx.png)
Figure 4 shows the NFVnice scheduling that makes the OS scheduler be governed by NF Manager via cgroups, and ultimately assigns running NFs to shared CPU cores.

*Activating NFs: *
It is critical to design the NF framework so that NFs are only activated when there are packets available for them to process, as is done in NFV platforms such as netmap and ClickOS.
While this provides an efficient mechanism for waking NFs, neither system allows for more complex resource management policies, which can lead to unfair CPU allocations across NFs, or inefficient scheduling across chains.

In NFVnice, NFs sleep by blocking on a semaphore shared with the NF Manager, granting the management plane great flexibility in deciding which NFs to activate at a given time. The policy we provide for activating an NF considers the number of packets pending in its queue, its priority relative to other NFs, and knowledge of the queue lengths of downstream NFs in the same chain. This allows the management framework to indirectly affect the CPU scheduling of NFs to be fairness and service-chain aware, without requiring that information be synchronized with the kernel’s scheduler.

*Relinquishing the CPU:*
After a batch of at most 32 packets is processed, libnf will check a shared memory flag set by the NF Manager that indicates if it should relinquish the CPU early (e.g., as a result of backpressure, as described below). If the flag is not set, the NF will attempt to process another batch; if the flag has been set or there are no packets available, the NF will block on the semaphore until notified by the Manager. 

*CPU Scheduler:*
Since multiple NF processes are likely to be in the runnable state at the same time, it is the operating system’s CPU scheduler that must determine which to run and for how long. In the early stages of our work we sought to design a custom CPU scheduler that would incorporate NF information such as queue lengths into its scheduling decisions. However, we found that synchronizing queue length information with the kernel, at the frequency necessary for NF scheduling, incurred overheads that outweighed any benefits.

In most cases, NFVnice NFs relinquish the CPU due to policies controlled by the manager, rather than through an involuntary context switch. This reduces overhead and helps NFVnice prioritize the most important NF for processing without requiring information sharing between user and kernel space.

*Assigning CPU Weights:*
NFVnice provides mechanisms to monitor a network function to estimate its CPU requirements, and to adjust its scheduling weight. 

we maintain a histogram of timings, allowing NFVnice to efficiently estimate the service time at different percentiles.

For each NF $i$ on a shared core, we calculate $load(i) = λi ∗ si$, the product of arrival rate, $λ$, and service time, $s$.
We then find the total load on each core, such as core m,
![](https://i.imgur.com/ap1OoI6.png)
and assign cpu shares for NFi on corem following the formula:
![](https://i.imgur.com/upxQDvR.png)
### C. Backpressure
![](https://i.imgur.com/yhgHY2i.png)
**A key goal of NFVnice is to avoid wasting work**, preventing an upstream NF from processing packets if they are just going to be dropped at a downstream NF later in the chain that has become overloaded.

*Cross-Chain Backpressure:*
The NF Manager is in an ideal position to observe behavior across NFs since it assists in moving packets between them.
When one of the NF Manager’s TX threads detects that the receive queue for an NF is above a high watermark (HIGH_WATER_MARK) and queuing time is above threshold, then it examines all packets in the NF’s queue to determine what service chain they are a part of. NFVnice then enables service chain-specific packet dropping at the upstream NFs. 

NF Manager maintains states of each NF, and in this case, it moves the NF’s state from backpressure watch list to packet throttle as shown in Figure 5. When the queue length becomes less than a low watermark (LOW_WATER_MARK), the state moves to clear throttle.

![](https://i.imgur.com/2FIV1oS.png)
The backpressure operation is illustrated in Figure 6, where four service chains (A-D) pass through several different NFs. The bold NFs (3 and 5) are currently overloaded. The NF Manager detects this and applies back pressure to flows A, C, and D. This is performed upstream where those flows first enter the system, minimizing wasted work.

*Local Optimization and ECN:*
NFVnice also supports simple, local backpressure, i.e., an NF will block if its output TX queue becomes full. This can happen because the NF Manager TX Thread responsible for the queue is overloaded.

Local backpressure is entirely NF-driven, and requires no coordination with the manager, so we use it to handle short bursts and cases where the manager is overloaded.

To facilitate congestion control across machines, the NF Manager will also mark the ECN bits in TCP flows in order to facilitate end-to-end management.

### D. Facilitating I/O
NF implementations should use asynchronous I/O to overlap packet processing with background I/O to maintain throughput. 
## IV. SIMULATION RESULTS
![](https://i.imgur.com/AWfWcpz.png)

![](https://i.imgur.com/vSAXcC2.png)

![](https://i.imgur.com/0SkfLgv.png)

![](https://i.imgur.com/3irAqsT.png)

![](https://i.imgur.com/pRhHCaO.png)

![](https://i.imgur.com/7rjquNn.png)

![](https://i.imgur.com/yMEsp1V.png)

![](https://i.imgur.com/ydS4Ro0.png)




